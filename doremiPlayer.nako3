/*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*
　ドレミプレーヤー-再生専用(doremiPlayer.nako3)

　Web Audio APIで、ドレミ演奏データ(JSONさん)を再生するだけのやつ
　doremi.nako3の演奏命令だけを抜き出したもの
*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*/

#------------------------------------------------
＃ ドレミ演奏命令
#------------------------------------------------
●(ドレミを|ドレミの)ドレミ演奏
    A＝オーディオコンテキスト作成。
    もし、(ドレミの変数型確認)≠「object」ならば、「MMLを直接演奏する場合は『doremi.nako3』を使用してください」と表示。
    Aでドレミをドレミデータ演奏。
    Aを戻す。
ここまで。

# Aはオーディオコンテキストを指定する
●(Aを|Aの)ドレミ演奏停止
    もし、(A≠NULL)ならば、:
        もし、(A["state"]≠「closed」)ならば、:
            Aの「close」を[]でJSメソッド実行。
ここまで。

●(Aを|Aの)ドレミ演奏一時停止
    もし、A≠NULLならば、Aの「suspend」を[]でJSメソッド実行。
ここまで。

●(Aを|Aの)ドレミ演奏再開
    もし、A≠NULLならば、Aの「resume」を[]でJSメソッド実行。
ここまで。
#------------------------------------------------
＃＃ドレミ演奏の内容
# まずオーディオコンテキストを作る
●オーディオコンテキスト作成
    「new AudioContext()||webkitAudioContext()」をJS実行。
ここまで。

# 演奏データを再生する
# Aはオーディオコンテキスト
●(Aでドレミデータを|ドレミデータの)ドレミデータ演奏
    ドレミデータのドレミ情報描画。
    ドレミデータ["データ"]を反復:
      もし、対象["音量"]＞0ならば、:
        音番号＝対象["音番号"]。
        開始時間＝対象["開始時間"]。
        停止時間＝対象["停止時間"]。
        余韻＝対象["音色"]の余韻長さ計算処理。
        オシレータ＝Aに対象で音色準備処理。
        オシレータ["frequency"]["value"]＝音番号を周波数変換処理。
        オシレータの「start」を[開始時間]でJSメソッド実行。
        オシレータの「stop」を[停止時間+余韻]でJSメソッド実行。
    Aとドレミデータのドレミ演奏タイマー。
ここまで。
#------------------------------------------------
＃ドレミデータ演奏の下請け
＃＃タイマー
●(Aとドレミデータの)ドレミ演奏タイマー
    Cとは変数。字幕データとは変数。
    もし、(ドレミデータ["情報"]["字幕設定"]≠NULL)かつ(ドレミデータ["字幕データ"]≠NULL)ならば、:
        C＝ドレミデータ["情報"]["字幕設定"]["カウンタ"]。
        字幕データ＝ドレミデータ["字幕データ"][C]。
    もし、A＝NULLならば、:
        もし、字幕データ≠NULLならば、ドレミデータ["情報"]["字幕設定"]["カウンタ"]＝0。
        戻る。
    違えば、もし、A["currentTime"]＞(ドレミデータ["情報"]["曲長"]＋2)ならば、:
        Aをドレミ演奏停止。
        戻る。
    違えば、もし、字幕データ≠NULLならば、:
        もし、A["currentTime"]＞字幕データ["時間"]ならば、:
            ドレミデータのCをドレミ字幕描画。
            ドレミデータ["情報"]["字幕設定"]["カウンタ"]を１増やす。。
    0.01秒待つ。
    Aとドレミデータのドレミ演奏タイマー。
ここまで。

＃＃字幕描画
●(ドレミデータのNoを)ドレミ字幕描画
    変数 字幕設定＝ドレミデータ["情報"]["字幕設定"]。
    変数 字幕データ＝ドレミデータ["字幕データ"][No]
    変数 x＝字幕設定["描画X"]。変数 y＝字幕設定["描画Y"]。
    変数 文字＝字幕データ["文字"]。変数 改行数＝字幕データ["改行"]。
    字幕設定["歌詞"]["カラオケ文字色"]に塗り色設定。
    字幕設定["歌詞"]["文字サイズ"]に描画フォント設定。
    変数 文字幅＝文字の文字幅取得。
    変数 文字高さ＝「あ」の文字高さ取得。
    文字高さ＝文字高さ+文字高さ/4。
    [x,y]へ文字を文字描画。
    字幕設定["描画X"]＝x+文字幅。
    もし、改行数＞０ならば、:
        字幕設定["描画X"]＝字幕設定["歌詞"]["X"]。
        字幕設定["描画Y"]＝y+文字高さ*改行数。
ここまで。

●(ドレミデータの)ドレミ情報描画
    変数 ドレミ情報＝ドレミデータ["情報"]
    変数 字幕設定＝ドレミ情報["字幕設定"]。
    もし、字幕設定＝NULLならば、戻る。
    「曲名,作者」を「,」で区切って反復:
        もし、ドレミ情報[対象]＝NULLならば、続ける。
        変数 x＝字幕設定[対象]["X"]。変数 y＝字幕設定[対象]["Y"]。
        字幕設定[対象]["文字色"]に塗り色設定。
        字幕設定[対象]["文字サイズ"]に描画フォント設定。
        [x,y]へドレミ情報[対象]を文字描画。
    「コメント,歌詞」を「,」で区切って反復:
        もし、ドレミ情報[対象]＝NULLならば、続ける。
        x＝字幕設定[対象]["X"]。y＝字幕設定[対象]["Y"]。
        字幕設定[対象]["文字色"]に塗り色設定。
        字幕設定["対象"]["文字サイズ"]に描画フォント設定。
        変数 文字高さ＝「あ」の文字高さ取得。
        文字高さ＝文字高さ+文字高さ/4。
        ドレミ情報[対象]を改行で区切って反復:
            [x,y]へ対象を文字描画。
            y＝y+文字高さ。
ここまで。

＃＃＃文字描画幅取得
●(Aの)文字幅取得
　　TM＝Aの文字描画幅取得。
　　TM["width"]で戻る。
ここまで。
●(Aの)文字高さ取得
　　TM＝Aの文字描画幅取得。
　　TM["actualBoundingBoxAscent"]+TM["actualBoundingBoxDescent"]で戻る。
ここまで。

＃＃ウェブオーディオAPI
●(オーディオコンテキストにデータで)音色準備処理とは
	オシレータ＝オーディオコンテキストの「createOscillator」を[]でJSメソッド実行。
	ゲイン＝オーディオコンテキストの「createGain」を[]でJSメソッド実行。
    # 音量設定
    音量＝データ["音量"]。
    開始時間＝データ["開始時間"]。
    停止時間＝データ["停止時間"]。
    ゲイン["gain"]の「setValueAtTime」を[音量,開始時間]でJSメソッド実行。
    # 音色設定
    //音色は10～19までサイン波、20～29まで矩形波、30～39まで鋸歯状波、40～49まで三角波。
    //0～9まで三角波(とりあえず三角波をデフォルトにする)
    //各1の位0～4まで単に余韻の長さの変化。5～9は発声時間固定で余韻の長さの変化。
    音色＝データ["音色"]。
    余韻＝音色の余韻長さ計算処理。
    オシレータ["type"]＝「triangle」//デフォルト
    もし、(10≦音色)かつ(音色＜20)ならば、オシレータ["type"]＝「sine」。
    もし、(20≦音色)かつ(音色＜30)ならば、オシレータ["type"]＝「square」。
    もし、(30≦音色)かつ(音色＜40)ならば、オシレータ["type"]＝「sawtooth」。
    もし、(40≦音色)かつ(音色＜50)ならば、オシレータ["type"]＝「triangle」。

    もし、((音色%10/5)を整数変換)＝0ならば、:
        ゲイン["gain"]の「setValueAtTime」を[音量,停止時間]でJSメソッド実行。
        ゲイン["gain"]の「exponentialRampToValueAtTime」を[0.001,停止時間+余韻]でJSメソッド実行。
    違えば、:
        発声時間＝0.5。
        音長＝停止時間-開始時間。
        もし、発声時間＞音長ならば、発声時間＝音長。
        ゲイン["gain"]の「exponentialRampToValueAtTime」を[0.001,開始時間+発声時間+余韻]でJSメソッド実行。
    # 接続
    オシレータの「connect」を[ゲイン]でJSメソッド実行。
    それの「connect」を[オーディオコンテキスト["destination"]]でJSメソッド実行。
    オシレータを戻す。
ここまで。

●(音色の)余韻長さ計算処理とは
    余韻＝(音色%5)/2。もし、余韻＝0ならば、余韻＝0.1。
    余韻を戻す。
ここまで。

＃＃音番号を周波数に変換する式
●(音番号を)周波数変換処理とは
    440*(2^((音番号-69)/12))で戻る。
ここまで。
#------------------------------------------------